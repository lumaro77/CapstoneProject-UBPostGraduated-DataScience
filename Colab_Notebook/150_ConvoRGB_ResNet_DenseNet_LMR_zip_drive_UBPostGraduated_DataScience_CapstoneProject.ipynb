{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUPA4cIYDda_"
      },
      "source": [
        "# **Project name: Endoscopic Capsule -- 150_ConvoRGB_ResNet_DenseNet_LMR_zip**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP4A14qISFCM"
      },
      "source": [
        "# **00. Project configuration**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBzW93W4kHLD"
      },
      "source": [
        "## 00.01 General configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKIzIYbJsNJc"
      },
      "outputs": [],
      "source": [
        "## Project folder\n",
        "collab_path = '/content/'\n",
        "project_path = collab_path\n",
        "training_folder = 'TRAIN'\n",
        "val_folder = 'VAL'\n",
        "\n",
        "## Imagery folders\n",
        "imagery_path = ''\n",
        "raw_imagery_path = '/raw/'\n",
        "\n",
        "## Drive folder\n",
        "drive_path = '/content/drive'\n",
        "drive_model_path = '/model/'\n",
        "drive_log_path = '/log/'\n",
        "drive_csv_path = '/csv_log/'\n",
        "\n",
        "## Project dataset\n",
        "file_id =\"1aeG-twXlUW2_d87TF7b_tKT3MH4GVcj3\"\n",
        "file_name = \"LMR_Capstone_Delivery.zip\"\n",
        "image_size = 100\n",
        "image_bands = 3\n",
        "num_classes = 11\n",
        "\n",
        "## Training mode\n",
        "single_dataset = True\n",
        "num_samples_train = 0\n",
        "num_samples_val = 0\n",
        "validation_fraction = 0\n",
        "retrain = False\n",
        "validation_file = ''\n",
        "weighted_train = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00.02 Training mode"
      ],
      "metadata": {
        "id": "vbVvPHtmRsGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################\n",
        "############################\n",
        "\n",
        "evaluating = False\n",
        "retrain = False\n",
        "\n",
        "mode01=True\n",
        "mode11=False\n",
        "mode21=False\n",
        "\n",
        "## Training\n",
        "if (not evaluating):\n",
        "  num_epochs  = 100\n",
        "\n",
        "############################\n",
        "############################\n",
        "batch_size  = 32\n",
        "learning_rate = 0.0005\n",
        "min_learning_rate = learning_rate / 100\n",
        "patience = num_epochs\n",
        "momentum = 0.9"
      ],
      "metadata": {
        "id": "LoC4z_JBPRl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 00.03 raining configuration"
      ],
      "metadata": {
        "id": "kRfhV1Vya5O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (evaluating):\n",
        "  num_epochs  = 0\n",
        "\n",
        "retrain = retrain or evaluating\n",
        "model_name = \"model_cnn_lumaro.h5\"\n",
        "drive_project_path = '/MyDrive/ML'\n",
        "\n",
        "#Mode 01\n",
        "if (mode01):\n",
        "  drive_project_path = '/MyDrive/ML_150_ConvoRGB_mode01'\n",
        "  single_dataset = True\n",
        "  weighted_train = False\n",
        "  training_file = 'split_all.csv'\n",
        "  validation_fraction = .3\n",
        "\n",
        "#Mode 11\n",
        "if (mode11):\n",
        "  drive_project_path = '/MyDrive/ML_150_ResNet_mode11'\n",
        "  single_dataset = True\n",
        "  weighted_train = False\n",
        "  training_file = 'split_all.csv'\n",
        "  validation_fraction = .3\n",
        "\n",
        "#Mode 21\n",
        "if (mode21):\n",
        "  drive_project_path = '/MyDrive/ML_150_DenseNet_mode21'\n",
        "  single_dataset = True\n",
        "  weighted_train = False\n",
        "  training_file = 'split_all.csv'\n",
        "  validation_fraction = .3"
      ],
      "metadata": {
        "id": "g7Ht9NfcalKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHfq3KHKO9V3"
      },
      "source": [
        "## 00.04 Loading Modules and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaRSpc-1Aw_2"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dgd6zBPiO9V3"
      },
      "outputs": [],
      "source": [
        "import os                                               # OS module in Python provides a way of using operating system dependent functionality\n",
        "\n",
        "import pandas as pd                                     # Data analysis and manipultion tool\n",
        "import numpy as np                                      # Fundamental package for linear algebra and multidimensional arrays\n",
        "import tensorflow as tf                                 # Deep Learning Tool\n",
        "import tensorflow_addons as tfa\n",
        "import cv2                                              # Library for image processing\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.applications.resnet import ResNet152,ResNet50\n",
        "from tensorflow.keras.applications.densenet import DenseNet169\n",
        "\n",
        "import sklearn.metrics as mtc\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from psutil import virtual_memory\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6SP0vc0PBTH"
      },
      "source": [
        "## 00.05 Showing Environtment configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMpFDIPMNytS"
      },
      "outputs": [],
      "source": [
        "print(\"Tensorflow version: \",tf.version.VERSION)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('(Not using a high-RAM runtime)')\n",
        "else:\n",
        "  print('(You are using a high-RAM runtime!)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYHd9xpXEtYO"
      },
      "source": [
        "## 00.06 Setting-up (Google-Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aket1CC2U8g4"
      },
      "outputs": [],
      "source": [
        "drive.mount(drive_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCu8pjW_mrvv"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(drive_path+drive_project_path):\n",
        "  os.makedirs(drive_path+drive_project_path)\n",
        "if not os.path.exists(drive_path+drive_project_path+drive_model_path):\n",
        "  os.makedirs(drive_path+drive_project_path+drive_model_path)\n",
        "if not os.path.exists(drive_path+drive_project_path+drive_log_path):\n",
        "  os.makedirs(drive_path+drive_project_path+drive_log_path)\n",
        "if not os.path.exists(drive_path+drive_project_path+drive_csv_path):\n",
        "  os.makedirs(drive_path+drive_project_path+drive_csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqhSlE_U0jW"
      },
      "source": [
        "## 00.07 Setting-up the environtment (Collab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKnExBnwstS9"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(project_path):\n",
        "  os.makedirs(project_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lWFveMRF24V"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(file_name):\n",
        "  !pip install gdown\n",
        "  !gdown --id $file_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4g2uAvqeb8V"
      },
      "outputs": [],
      "source": [
        "#unzip the data\n",
        "dir = project_path+imagery_path\n",
        "!unzip -q -n $file_name -d $dir\n",
        "dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u3zhrpt9pi2"
      },
      "source": [
        "# **01. Loading and preparing training data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 01.01 Recovering info from training file"
      ],
      "metadata": {
        "id": "PHylLkqWsg4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np7jefkV9pi3"
      },
      "outputs": [],
      "source": [
        "file = project_path+imagery_path+training_file\n",
        "file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU5KAuhC9pi4"
      },
      "outputs": [],
      "source": [
        "labels = pd.read_csv(file)   # loading the labels\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdsON8gZfKIV"
      },
      "outputs": [],
      "source": [
        "dir = project_path+imagery_path+'raw/'\n",
        "dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxpQGP4ZZChh"
      },
      "outputs": [],
      "source": [
        "file_paths = [dir + fname for fname in labels['filename']]\n",
        "file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4YrF-s1rWla"
      },
      "outputs": [],
      "source": [
        "# Confirm if number of images is same as number of labels given\n",
        "if len(labels) == len(file_paths):\n",
        "    print('Number of labels i.e. ', len(labels), 'matches the number of filenames i.e. ', len(file_paths))\n",
        "else:\n",
        "    print('Number of labels does not match the number of filenames')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx2t_cLSr3Jm"
      },
      "source": [
        "#### Adding the file_paths to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8CNnxxzsG56"
      },
      "outputs": [],
      "source": [
        "train_data = labels\n",
        "train_data['filepaths'] = file_paths\n",
        "print(\"Num of train_data: \",len(train_data))\n",
        "train_data      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68JvUTtb3EmO"
      },
      "source": [
        "## 01.02 Plotting some training images\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image,label):\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True,figsize=(12,10))\n",
        "  orig_img = cv2.imread(image)\n",
        "  orig_img = orig_img[..., ::-1]\n",
        "  processed_img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "  processed_img_2 = cv2.cvtColor(orig_img, cv2.COLOR_BGR2HSV)\n",
        "  ax1.imshow(orig_img)\n",
        "  ax2.imshow(processed_img, cmap='gray', vmin=0, vmax=255)\n",
        "  ax3.imshow(processed_img_2)\n",
        "  ax1.set_title(f'RGB Image class {label}')\n",
        "  ax2.set_title(f'Gray Image class {label}')\n",
        "  ax3.set_title(f'HSV Image class {label}')"
      ],
      "metadata": {
        "id": "cKvPZ-kR3EmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_labels = np.unique([label for label in labels['label']])\n",
        "vec_labels"
      ],
      "metadata": {
        "id": "O5YhBnJIjDa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in vec_labels:\n",
        "  sub_list = train_data[train_data['label'] == item].sample().reset_index()\n",
        "  show_image(sub_list['filepaths'][0],sub_list['label'][0])"
      ],
      "metadata": {
        "id": "kL1uja8n3EmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkD1sGh1qU0_"
      },
      "source": [
        "\n",
        "## 01.03 Make Folders for training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB2C41HKqaQc"
      },
      "outputs": [],
      "source": [
        "_dir_train = os.path.join(project_path,training_folder)\n",
        "_dir_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LcB5yumqkiF"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(_dir_train):\n",
        "  shutil.rmtree(_dir_train)\n",
        "os.path.exists(_dir_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u1ZBwoZqsRO"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(_dir_train):\n",
        "    os.makedirs(_dir_train)\n",
        "_dir_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e32uEp3ucGi"
      },
      "outputs": [],
      "source": [
        "labels = train_data['label'].unique().tolist()\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVUYMROBq6g4"
      },
      "outputs": [],
      "source": [
        "for label in labels:\n",
        "  _dir = os.path.join(project_path,training_folder,label)\n",
        "  if not os.path.exists(_dir):\n",
        "    os.makedirs(_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMW72jd9yFtn"
      },
      "source": [
        "## 01.04 Copy Training images to respective folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yGgu5c8yEyC"
      },
      "outputs": [],
      "source": [
        "if (num_samples_train == 0):\n",
        "  for i in tqdm(train_data.index):\n",
        "    images = train_data[\"filepaths\"][i]\n",
        "    lbls = train_data[\"label\"][i]\n",
        "    dest = os.path.join(project_path,training_folder,lbls)\n",
        "    if os.stat(images).st_size != 0:\n",
        "      shutil.copy(images,dest)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = train_data['label'].unique().tolist()\n",
        "labels"
      ],
      "metadata": {
        "id": "1yn8Inz4ByaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = train_data.label.value_counts()\n",
        "counts"
      ],
      "metadata": {
        "id": "mq1xbL_402ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (num_samples_train > 0):\n",
        "  for label in tqdm(labels):\n",
        "    clas_data = train_data[train_data['label']==label]\n",
        "    clas_data = clas_data.sample(frac=1)\n",
        "    lap = 1000\n",
        "    num = num_samples_train\n",
        "    while (num > 0):\n",
        "      for i in clas_data.index:\n",
        "        image = clas_data[\"filepaths\"][i]\n",
        "        lbl = clas_data[\"label\"][i]\n",
        "        name = str(lap)+'_'+os.path.basename(image)\n",
        "        dest = os.path.join(project_path,training_folder,lbl,name)\n",
        "        if os.stat(image).st_size != 0:\n",
        "          if (num > 0):\n",
        "            shutil.copy(image,dest)\n",
        "            num = num - 1\n",
        "      lap = lap + 1\n"
      ],
      "metadata": {
        "id": "NsPG96M_wKhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znobaA5KsXgp"
      },
      "source": [
        "## 01.05 Statistics of training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb8DPSQ1SOil"
      },
      "outputs": [],
      "source": [
        "train_data['label'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MpWrTiMw138"
      },
      "outputs": [],
      "source": [
        "train_data.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4YQATk_ZB2Q"
      },
      "outputs": [],
      "source": [
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.xticks(rotation =90)\n",
        "sns.countplot(train_data.label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yvdy5zghUn4"
      },
      "outputs": [],
      "source": [
        "def f(r):\n",
        "  if r == \"Angiectasia\":\n",
        "    return 0\n",
        "  elif r == \"Blood_fresh\":\n",
        "    return 1\n",
        "  elif r == \"Erosion\":\n",
        "    return 2\n",
        "  elif r == \"Erythematous\":\n",
        "    return 3\n",
        "  elif r == \"Foreign_body\":\n",
        "    return 4\n",
        "  elif r == \"Ileocecal_valve\":\n",
        "    return 5\n",
        "  elif r == \"Lymphangiectasia\":\n",
        "    return 6\n",
        "  elif r == \"Normal\":\n",
        "    return 7\n",
        "  elif r == \"Pylorus\":\n",
        "    return 8\n",
        "  elif r == \"Reduced_mucosal_view\":\n",
        "    return 9\n",
        "  else:\n",
        "    return 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6W6l4GFneaO"
      },
      "outputs": [],
      "source": [
        "train_data['label'] = train_data['label'].apply(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0DbPKB2SQS3"
      },
      "outputs": [],
      "source": [
        "labels = train_data['label'].unique().tolist()\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTtuxMeIZFFZ"
      },
      "outputs": [],
      "source": [
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.xticks(rotation =90)\n",
        "sns.countplot(train_data.label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlI0PCxe49Op"
      },
      "source": [
        "## 01.06 Training Dataset creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kuEtQlX5SN3"
      },
      "outputs": [],
      "source": [
        "if (not single_dataset):\n",
        "  subset = None\n",
        "  validation_split = 0\n",
        "else:\n",
        "  subset = 'training'\n",
        "  validation_split = validation_fraction\n",
        "\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    project_path + training_folder,\n",
        "    labels = \"inferred\",\n",
        "    label_mode = \"categorical\",\n",
        "    class_names = None,\n",
        "    color_mode = \"rgb\",\n",
        "    batch_size = batch_size,\n",
        "    image_size = (image_size, image_size),\n",
        "    shuffle = True,\n",
        "    seed = 42,\n",
        "    validation_split = validation_split,\n",
        "    subset = subset,\n",
        "    interpolation = \"bilinear\",\n",
        "    follow_links = False,\n",
        "    crop_to_aspect_ratio = False\n",
        ")\n",
        "\n",
        "train_dataset_len = len(train_dataset.file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzUEDdHdjoae"
      },
      "source": [
        "# **02. Loading and preparing validation data** (as a fraction of main dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEir5y836ijQ"
      },
      "source": [
        "## 02.01 Validation Dataset creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2K7qusP6ijR"
      },
      "outputs": [],
      "source": [
        "if (single_dataset):\n",
        "  val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "      project_path + training_folder,\n",
        "      labels = \"inferred\",\n",
        "      label_mode = \"categorical\",\n",
        "      class_names = None,\n",
        "      color_mode = \"rgb\",\n",
        "      batch_size = batch_size,\n",
        "      image_size = (image_size, image_size),\n",
        "      shuffle = True,\n",
        "      seed = 42,\n",
        "      validation_split = validation_fraction,\n",
        "      subset = \"validation\",\n",
        "      interpolation = \"bilinear\",\n",
        "      follow_links = False,\n",
        "      crop_to_aspect_ratio = False\n",
        "  )\n",
        "  val_dataset_len = len(val_dataset.file_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sWPyXcgoorw"
      },
      "source": [
        "# **03. Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03.01 Prefetched and cached datasets"
      ],
      "metadata": {
        "id": "4WMuaKYes9Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "pNJflFoHs9Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43UVCtwzVLun"
      },
      "source": [
        "## 03.02 Definition of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic convolutional model (Case 01)"
      ],
      "metadata": {
        "id": "ODWRjRl2esmK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDq3uIUegG_p"
      },
      "outputs": [],
      "source": [
        "if (mode01):\n",
        "  model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Input([image_size, image_size, 3], dtype = tf.uint8),\n",
        "      tf.keras.layers.Rescaling(scale=1./127.5, offset=-1),\n",
        "      tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
        "      tf.keras.layers.RandomFlip(mode=\"vertical\", seed=42),\n",
        "      tf.keras.layers.RandomRotation(1, fill_mode=\"reflect\", interpolation=\"bilinear\", seed=42, fill_value=0.0),\n",
        "      tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D((2, 2)), \n",
        "      tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "      tf.keras.layers.BatchNormalization(),\n",
        "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.20),\n",
        "      tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet based model (Case 11)"
      ],
      "metadata": {
        "id": "U52kQz8ae1ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (mode11):\n",
        "  pre_trained_model = ResNet152(include_top=False, weights= 'imagenet', pooling='avg')\n",
        "  for layer in pre_trained_model.layers:  \n",
        "      layer.trainable=False\n",
        "\n",
        "  inputs = tf.keras.layers.Input((image_size, image_size, 3))\n",
        "\n",
        "  x = tf.keras.applications.resnet50.preprocess_input(inputs) # Preprocessing layer, normalization -1 1\n",
        "\n",
        "  x = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42)(x)\n",
        "  x = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=42)(x)\n",
        "  x = tf.keras.layers.RandomRotation(1, fill_mode=\"reflect\", interpolation=\"bilinear\", seed=42, fill_value=0.0)(x)\n",
        "\n",
        "  x = pre_trained_model(x)\n",
        "\n",
        "  x = tf.keras.layers.Dropout(0.4)(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)   \n",
        "  out = tf.keras.layers.Dense(num_classes, activation='softmax')(x)           \n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=out) "
      ],
      "metadata": {
        "id": "edBVRxnadqmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet based model (Case 21)"
      ],
      "metadata": {
        "id": "kj7RViGBe-u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (mode21):\n",
        "  pre_trained_model = DenseNet169(include_top=False, weights= 'imagenet', pooling='avg')\n",
        "  for layer in pre_trained_model.layers:  \n",
        "      layer.trainable=False\n",
        "\n",
        "  inputs = tf.keras.layers.Input((image_size, image_size, 3))\n",
        "  x = tf.keras.applications.densenet.preprocess_input(inputs) # Preprocessing layer, normalization -1 1\n",
        "  x = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42)(x)\n",
        "  x = tf.keras.layers.RandomFlip(mode=\"vertical\", seed=42)(x)\n",
        "  x = tf.keras.layers.RandomRotation(1, fill_mode=\"reflect\", interpolation=\"bilinear\", seed=42, fill_value=0.0)(x)\n",
        "  x = pre_trained_model(x)\n",
        "  x = tf.keras.layers.Dropout(0.4)(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  out = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=out) "
      ],
      "metadata": {
        "id": "mDsG10G1dqW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Summary"
      ],
      "metadata": {
        "id": "eW7ox3pNyKKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7t0Z74VZSABb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Plot"
      ],
      "metadata": {
        "id": "vlb9C4hvySNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wsj02mmiLjb"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03.03 Model Compilation"
      ],
      "metadata": {
        "id": "RDZr2ZSheTUC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQWMyQLva53K"
      },
      "outputs": [],
      "source": [
        "loss_weights=None\n",
        "\n",
        "model.compile(optimizer = tf.optimizers.SGD(learning_rate=learning_rate,momentum=momentum),\n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "              loss_weights = loss_weights,\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy(),\n",
        "                       tf.keras.metrics.CategoricalCrossentropy(),\n",
        "                       tf.keras.metrics.FalseNegatives(),\n",
        "                       tf.keras.metrics.FalsePositives(), \n",
        "                       tf.keras.metrics.TrueNegatives(),\n",
        "                       tf.keras.metrics.TruePositives(), \n",
        "                       tfa.metrics.F1Score(num_classes=num_classes, average=\"micro\"), \n",
        "                       tf.keras.metrics.Precision(), \n",
        "                       tf.keras.metrics.Recall(), \n",
        "                       tfa.metrics.MatthewsCorrelationCoefficient(num_classes=num_classes)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03.04 Setting up callbacks"
      ],
      "metadata": {
        "id": "UOUTByIpvFcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "MoreCallbacks\n",
        "EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min'),\n",
        "ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min',min_delta=1e-4)\n",
        "'''"
      ],
      "metadata": {
        "id": "TR9926tkIyR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEQhSY4pKBoC"
      },
      "outputs": [],
      "source": [
        "model_filepath = drive_path + drive_project_path + drive_model_path +  model_name\n",
        "model_filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVExpCQ1QyZ7"
      },
      "outputs": [],
      "source": [
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=model_filepath,\n",
        "                                                 save_best_only=True,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FNKflPZQm3I"
      },
      "outputs": [],
      "source": [
        "csv_log = drive_path+drive_project_path+drive_csv_path+\"csv_log.csv\"\n",
        "csv_log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPO7sCTsKLNZ"
      },
      "outputs": [],
      "source": [
        "if retrain:\n",
        "  model.load_weights(model_filepath)\n",
        "  csvlog_callback = tf.keras.callbacks.CSVLogger(csv_log, separator=',', append=True)\n",
        "  train_history = pd.read_csv(csv_log)\n",
        "  initial_epoch = 1+(train_history['epoch'].tail(1).tolist())[0]\n",
        "else:\n",
        "  initial_epoch = 0\n",
        "  csvlog_callback = tf.keras.callbacks.CSVLogger(csv_log, separator=',', append=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXSLlZG0H562"
      },
      "outputs": [],
      "source": [
        "log_filepath = drive_path + drive_project_path + drive_log_path \n",
        "log_filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylsp5uSR9h7N"
      },
      "outputs": [],
      "source": [
        "log_callback = tf.keras.callbacks.TensorBoard(log_filepath, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "terminateOnNaN = tf.keras.callbacks.TerminateOnNaN()"
      ],
      "metadata": {
        "id": "WFtfncuosyn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', \n",
        "                                                  factor=0.5,\n",
        "                                                  patience=patience, \n",
        "                                                  min_lr=min_learning_rate)"
      ],
      "metadata": {
        "id": "WCrs_duptfzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhM_GLYxiM2d"
      },
      "source": [
        "## 03.05 Fiting the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzrKjkinAgrc"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir $log_filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUGnikPScUeO"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi -L\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJhh_QYT808g"
      },
      "outputs": [],
      "source": [
        "class_weight=None\n",
        "\n",
        "if (num_epochs > 0):\n",
        "  train_history = model.fit(train_dataset,\n",
        "                          initial_epoch=initial_epoch,\n",
        "                          epochs=initial_epoch+num_epochs,\n",
        "                          batch_size=batch_size,\n",
        "                          validation_data=val_dataset,\n",
        "                          class_weight=class_weight,\n",
        "                          callbacks=[reduce_lr, cp_callback, log_callback, csvlog_callback, terminateOnNaN])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjnj-rHXspsh"
      },
      "outputs": [],
      "source": [
        "if (num_epochs > 0):\n",
        "  for key in train_history.history.keys():\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjTRtyEqH5v0"
      },
      "outputs": [],
      "source": [
        "#PLOT HISTORY\n",
        "def show_history(history, label_a, name_a=None, label_b=None, name_b=None):\n",
        "  plt.plot(history[label_a],label=name_a)\n",
        "  if (label_b != None):\n",
        "    plt.plot(history[label_b],label=name_b)\n",
        "  plt.xlabel('Epochs')\n",
        "  if (name_b != None):\n",
        "    plt.ylabel(name_b)\n",
        "    plt.title(name_b+' vs Epochs')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def make_plots(history):\n",
        "  show_history(history, 'lr','Learning Rate')\n",
        "  show_history(history, 'val_loss','Validation Loss',\n",
        "                                      'loss','Training Loss')\n",
        "  show_history(history, 'val_categorical_accuracy','Validation Categorical Accuracy',\n",
        "                                      'categorical_accuracy','Training Categorical Accuracy')\n",
        "  show_history(history, 'val_categorical_crossentropy','Validation Categorical Crossentropy',\n",
        "                                      'categorical_crossentropy','Categorical Crossentropy')\n",
        "  show_history(history, 'val_MatthewsCorrelationCoefficient','Validation MatthewsCorrelationCoefficient',\n",
        "                                      'MatthewsCorrelationCoefficient','MatthewsCorrelationCoefficient')\n",
        "\n",
        "if (num_epochs > 0):\n",
        "  make_plots(train_history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Ig4L0IvETh"
      },
      "outputs": [],
      "source": [
        "csv_train_history = pd.read_csv(csv_log)\n",
        "for key in csv_train_history.keys():\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkWmCtkOvTjg"
      },
      "outputs": [],
      "source": [
        "#PLOT HISTORY\n",
        "make_plots(csv_train_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvS8qo83V0VQ"
      },
      "source": [
        "## 03.06 Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval = model.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "pv9HYWGJVxkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGi4ZJxnHb28"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('loss = {:.3f}'.format(eval[0]))\n",
        "print('categorical_accuracy = {:.3f}'.format(eval[1]))\n",
        "print('categorical_crossentropy = {:.3f}'.format(eval[2]))\n",
        "print('false_negatives = ',int(eval[3]))\n",
        "print('false_positives = ',int(eval[4]))\n",
        "print('true_negatives = ',int(eval[5]))\n",
        "print('true_positives = ',int(eval[6]))\n",
        "print('f1_score = {:.3f}'.format(eval[7]))\n",
        "print('precision = {:.3f}'.format(eval[8]))\n",
        "print('recall = {:.3f}'.format(eval[9]))\n",
        "print('MatthewsCorrelationCoefficient = {:.3f}'.format(eval[10]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxtDS6-0J0s2"
      },
      "source": [
        "# **04. Make Prediction and Analysis on Validation Dataset**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04.01 Prediction"
      ],
      "metadata": {
        "id": "piq5kUqab_R-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8U7w1dqewvE"
      },
      "outputs": [],
      "source": [
        "y_val = []\n",
        "prediction = []\n",
        "for images, labels in val_dataset:\n",
        "    for label in labels:\n",
        "      y_val.append(np.nanargmax(label.numpy()))\n",
        "    preds = model.predict(images)\n",
        "    for pred in preds:\n",
        "      prediction.append(np.nanargmax(pred)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_NNm-Et2l9B"
      },
      "outputs": [],
      "source": [
        "np.unique(y_val, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8waKEdrE2llq"
      },
      "outputs": [],
      "source": [
        "np.unique(prediction, return_counts=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04.02 Analisys and report"
      ],
      "metadata": {
        "id": "Xa8KKklIcI9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vVeV4STJvr3"
      },
      "outputs": [],
      "source": [
        "def test_model(y_true, y_predicted):\n",
        "    print(\"Accuracy = {:.3f}\".format(mtc.accuracy_score(y_true, y_predicted)))\n",
        "    print(\"Accuracy Balanced = {:.3f}\".format(mtc.balanced_accuracy_score(y_true, y_predicted)))\n",
        "    \n",
        "    print(\"Precision micro = {:.3f}\".format(mtc.precision_score(y_true,y_predicted, average=\"micro\")))\n",
        "    print(\"Precision macro = {:.3f}\".format(mtc.precision_score(y_true,y_predicted, average=\"macro\")))\n",
        "    print(\"Precision weighted = {:.3f}\".format(mtc.precision_score(y_true,y_predicted, average=\"weighted\")))\n",
        "    \n",
        "    print(\"Recall micro = {:.3f}\".format(mtc.recall_score(y_true, y_predicted, average=\"micro\")))\n",
        "    print(\"Recall macro = {:.3f}\".format(mtc.recall_score(y_true, y_predicted, average=\"macro\")))\n",
        "    print(\"Recall weighted = {:.3f}\".format(mtc.recall_score(y_true, y_predicted, average=\"weighted\")))\n",
        "\n",
        "    print(\"F1 micro = {:.3f}\".format(mtc.f1_score(y_true, y_predicted, average=\"micro\")))\n",
        "    print(\"F1 macro = {:.3f}\".format(mtc.f1_score(y_true, y_predicted, average=\"macro\")))\n",
        "    print(\"F1 weighted = {:.3f}\".format(mtc.f1_score(y_true, y_predicted, average=\"weighted\")))\n",
        "\n",
        "    print(\"MCC = {:.3f}\".format(mtc.matthews_corrcoef(y_true, y_predicted)))\n",
        "    print(\"Kappa = {:.3f}\".format(mtc.cohen_kappa_score(y_true, y_predicted)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LkpRKXoKuFd"
      },
      "outputs": [],
      "source": [
        "test_model(y_val, prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkp5xNoq76pv"
      },
      "outputs": [],
      "source": [
        "#classification_report\n",
        "report = classification_report(y_val, prediction)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04.03 Confusion Matrix"
      ],
      "metadata": {
        "id": "bmktyucvcR6R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g23ex2g0VeP"
      },
      "outputs": [],
      "source": [
        "# confusion matrix\n",
        "cm = pd.DataFrame(confusion_matrix(y_val, prediction))\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "cm = confusion_matrix(y_val, prediction, labels= list(range(num_classes)))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels= list(range(num_classes)))\n",
        "disp.plot(ax= ax)"
      ],
      "metadata": {
        "id": "aWamRegUjukx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11oyvZ1nEm9Q"
      },
      "outputs": [],
      "source": [
        "# confusion matrix plot\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "cm = confusion_matrix(y_val, prediction, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels= list(range(num_classes)))\n",
        "disp.plot(ax=ax)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "wUPA4cIYDda_",
        "XBzW93W4kHLD",
        "kRfhV1Vya5O2",
        "sHfq3KHKO9V3",
        "f6SP0vc0PBTH",
        "cYHd9xpXEtYO",
        "UKqhSlE_U0jW",
        "PHylLkqWsg4a",
        "Wx2t_cLSr3Jm",
        "68JvUTtb3EmO",
        "PkD1sGh1qU0_",
        "KMW72jd9yFtn",
        "znobaA5KsXgp",
        "tlI0PCxe49Op",
        "JEir5y836ijQ",
        "4WMuaKYes9Rt",
        "ODWRjRl2esmK",
        "U52kQz8ae1ri",
        "kj7RViGBe-u7",
        "eW7ox3pNyKKX",
        "RDZr2ZSheTUC",
        "UOUTByIpvFcL",
        "NhM_GLYxiM2d",
        "LvS8qo83V0VQ",
        "piq5kUqab_R-",
        "Xa8KKklIcI9s",
        "bmktyucvcR6R"
      ],
      "name": "150_ConvoRGB_ResNet_DenseNet_LMR_zip_drive_UBPostGraduated_DataScience_CapstoneProject.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}